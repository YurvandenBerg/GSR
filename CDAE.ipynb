{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten, Dropout, add, Activation\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import random \n",
    "import pickle\n",
    "import copy\n",
    "import numpy               as np\n",
    "np.random.seed(7)\n",
    "import pandas              as pd\n",
    "import scipy.sparse        as sparse\n",
    "import scipy.sparse.linalg as linalg\n",
    "from numpy              import mat\n",
    "from datetime           import datetime\n",
    "from tqdm               import tqdm\n",
    "from numpy.core.numeric import zeros_like\n",
    "from sklearn.metrics    import pairwise_distances\n",
    "from numpy              import linalg as la\n",
    "from datetime           import date   as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataProcessing:\n",
    " \n",
    "    folder = '/Users/Yur/Desktop/Thesis/Data/TaFeng.csv'\n",
    " \n",
    "    def __init__(self):\n",
    "        self.usersTest = {}\n",
    "        self.itemDict = {}\n",
    "        self.saveUsersTest = 'usersTest.dict'\n",
    "        self.saveItemDict = 'itemDict.dict'\n",
    "        self.saveUI = 'sUI.mtx'\n",
    "        self.saveBI = 'basketDict.dict'\n",
    " \n",
    "    def readTaFeng(self, folder):\n",
    "        '''\n",
    "        The TaFeng dataset posseses the useful information in cols 0, 1 and 4. \n",
    "        For other datasets, please change the usecols\n",
    " \n",
    "        Args: \n",
    "            folder: the path to the corresponding folder\n",
    "        Returns:\n",
    "            df: DataFrame with all the transaction data\n",
    "        '''\n",
    "        df = pd.read_csv(folder, usecols=[0,1,4])\n",
    "        df.columns = ['date_time', 'customer_id', 'subclass']\n",
    " \n",
    "        df['date_time'] = df['date_time'].apply(lambda x: datetime.strptime(x[:10], '%m/%d/%Y'))\n",
    "        return df\n",
    " \n",
    "    def addTransactionID(self, df):\n",
    "        \"\"\"\n",
    "        Create transaction id for each product purchased (row in df). Products in the same basket have the same transaction id.\n",
    " \n",
    "        Args:\n",
    "            df: Dataframe which includes all the product purchased.\n",
    "        Returns:\n",
    "            df: Add transaction id to the input df.\n",
    "        \"\"\"\n",
    "        df['trans_id'] =  df['customer_id'].astype(str) + df['date_time'].astype(str) # assume each customer only make at most one transaction everyday\n",
    "        df = df.sort_values(['trans_id']) # sort before finding the products in the same basket\n",
    "        cust_id = df.customer_id.values[0]\n",
    "        trans_buf_id = df.trans_id.values[0]\n",
    "        trans = 0\n",
    "        trans_id_list = []\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            if df.customer_id.values[i] == cust_id:\n",
    "                if df.trans_id.values[i] == trans_buf_id:\n",
    "                    trans_id_list.append(trans)\n",
    "                else:\n",
    "                    trans += 1\n",
    "                    trans_buf_id = df.trans_id.values[i]\n",
    "                    trans_id_list.append(trans)\n",
    "            else:\n",
    "                cust_id = df.customer_id.values[i]\n",
    "                trans_buf_id = df.trans_id.values[i]\n",
    "                trans = 0\n",
    "                trans_id_list.append(trans)\n",
    "        df['trans_id'] = trans_id_list\n",
    "        return df\n",
    " \n",
    "    def nTransactionsCustomers(self, new_df):\n",
    "        '''\n",
    "        Get the number of different days a customer has shopped groceries and use the last time as test basket\n",
    " \n",
    "        Args: \n",
    "            df: DataFrame which includes the transaction id's\n",
    "        Returns:\n",
    "            train_set: the test that will be used to train the similarity metrices\n",
    "            test_test: the baskets that will be used for testing, with customers that shopped at least twice\n",
    "        '''\n",
    "        df = new_df.sort_values(['customer_id', 'date_time', 'trans_id'], ascending=[True, False, False])\n",
    "        df_cust_purch = new_df.groupby(['customer_id'],as_index = False).trans_id.max() # number of purchases per user\n",
    "        cust_id  = df.customer_id.values[0]\n",
    "        n_purchases = df_cust_purch.trans_id.values[0]\n",
    " \n",
    "        purch_list = []\n",
    "        curr_cust = 0\n",
    "        for i in tqdm(range(df.shape[0])):\n",
    "            if df.customer_id.values[i] == cust_id:\n",
    "                purch_list.append(n_purchases)\n",
    "            else:\n",
    "                curr_cust = curr_cust + 1\n",
    "                cust_id = df.customer_id.values[i]\n",
    "                n_purchases = df_cust_purch.trans_id.values[curr_cust]\n",
    "                purch_list.append(n_purchases)\n",
    "        df['nb_purchases'] = purch_list\n",
    " \n",
    "        test_set = df.loc[df['trans_id'] == df['nb_purchases']]\n",
    "        train_set = df.loc[df['trans_id'] != df['nb_purchases']]\n",
    "        return train_set, test_set.loc[test_set['trans_id'] >0]\n",
    " \n",
    "    def aggregateUserProcessing(self, dataset):\n",
    "        '''\n",
    "        This method reads a training data set and converts it into a UI dataframe\n",
    "         \n",
    "        Args:\n",
    "            dataset: the training set (train_set from preceding function)\n",
    "        Returns:\n",
    "            UI: the User-Item Matrix\n",
    "            ItemDict: dictionary with item keys\n",
    "        '''\n",
    "        userDict = dict()\n",
    "        ind_u = 0\n",
    "        ind_i = 0\n",
    "        dataset = dataset.loc[:,['date_time', 'customer_id','subclass']].values.tolist()\n",
    "        print('Counting user and item...')\n",
    "        for d, u, i in dataset:\n",
    "            if int(u) not in userDict.keys():\n",
    "                userDict[int(u)] = ind_u\n",
    "                ind_u+=1\n",
    "            if i not in self.itemDict.keys():\n",
    "                self.itemDict[i] = ind_i\n",
    "                ind_i+=1\n",
    " \n",
    "        self.UI = np.zeros((len(userDict.keys()), len(self.itemDict.keys())))\n",
    " \n",
    "        print('Populating aggregate matrix')\n",
    "        for d, u, i in dataset:\n",
    "            self.UI[userDict[int(u)], self.itemDict[i]]+=1\n",
    " \n",
    "        # Log frequence UI transformation (TO BE OPTIMIZE)\n",
    "        # -------------------------------\n",
    "        print('Frequence normalisation')\n",
    "        for i,j in np.argwhere(self.UI!=0):\n",
    "            self.UI[i,j] = np.log(self.UI[i,j]+1)\n",
    " \n",
    "        self.UI = pd.DataFrame(data=self.UI,    # values\n",
    "                          index=userDict.keys(),    # 1st column as index\n",
    "                          columns=self.itemDict.keys())\n",
    " \n",
    "        self.UI = self.UI.div(self.UI.sum(axis=1), axis=0) #normalize rows \n",
    "  \n",
    "        return self.UI, self.itemDict\n",
    " \n",
    " \n",
    "    def aggregateTransactionProcessing(self, dataset, minLengthBasket):\n",
    "        '''\n",
    "        This method reads a test set and splits it into a list of baskets that have been purchases per customer\n",
    " \n",
    "        Args:\n",
    "            dataset: test data\n",
    "        returns:\n",
    "            self.BasketItemList: list of purchased baskets that has to be split into training and test baskets\n",
    "        '''\n",
    "        print('Create test baskets dictionnary')\n",
    "        dataset = dataset.values.tolist()\n",
    "        basketDict = dict()\n",
    "         \n",
    "        ind_b = 0\n",
    "        for i in np.arange(len(dataset)):\n",
    "            if (dataset[i][1]) not in basketDict.keys():\n",
    "                basketDict[dataset[i][1]] = ind_b\n",
    "                ind_b+=1\n",
    "         \n",
    "        print('Create basket array')\n",
    "        BasketItem = np.zeros((len(basketDict), len(self.itemDict.keys())), dtype='int')\n",
    "        for i in np.arange(len(dataset)):\n",
    "            if dataset[i][2] in self.itemDict.keys():\n",
    "                BasketItem[basketDict[dataset[i][1]], self.itemDict[dataset[i][2]]] +=1\n",
    "         \n",
    "        self.BasketItemList = []\n",
    "        self.usersTest = {y:x for x, y in basketDict.items()}\n",
    "        for i in range(BasketItem.shape[0]):\n",
    "            self.BasketItemList.append(np.argwhere(BasketItem[i,:]!=0).flatten())\n",
    "         \n",
    "        placeToProductDict = {y:x for x,y in self.itemDict.items()}\n",
    " \n",
    "        for i in range(len(self.BasketItemList)):\n",
    "            for j in range(len(self.BasketItemList[i])):\n",
    "                self.BasketItemList[i][j] = placeToProductDict[self.BasketItemList[i][j]]\n",
    " \n",
    "        index = 0\n",
    "        user = 0\n",
    "        correct_baskets = copy.copy(self.BasketItemList) \n",
    "        for basket in self.BasketItemList: # remove the purchases that have less than 4 separate products in them\n",
    "            if len(basket) < minLengthBasket:\n",
    "                correct_baskets.pop(index)\n",
    "                self.usersTest.pop(user)\n",
    "                user = user + 1\n",
    "            else:\n",
    "                index = index + 1\n",
    "                user = user + 1\n",
    "        self.BasketItemList = correct_baskets\n",
    "        \n",
    "        return self.BasketItemList, self.usersTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareCDAE(UI, baskets, usersTest, itemDict):\n",
    "    #UI[UI!=0] = 1\n",
    "    \n",
    "    train_x = UI.values\n",
    "    test_x = np.zeros(shape=(len(baskets), UI.shape[1]))\n",
    "\n",
    "    for i, r in enumerate(baskets):\n",
    "        for j, c in enumerate(itemDict.keys()):\n",
    "            test_x[i,j] = int(c in r)\n",
    "    \n",
    "    # split train into train and validation randomly\n",
    "    #train_validation_split = np.random.rand(len(train_x)) < 0.80\n",
    "    #validation_x = train_x[~train_validation_split]\n",
    "    #train_x      = train_x[train_validation_split]\n",
    "    \n",
    "#     # get users of train, val and test\n",
    "#     users_x = list(UI.index)\n",
    "#     userList = np.array(users_x).reshape(len(users_x), 1)\n",
    "#     users_train_x = userList[train_validation_split]\n",
    "#     users_val_x   = userList[~train_validation_split]\n",
    "    \n",
    "#     users_test_x = list(usersTest.values())\n",
    "#     users_test_x = np.array(users_test_x).reshape(len(users_test_x), 1)\n",
    "    return train_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create(I, U, K, hidden_activation, output_activation, q=0.2, l=0.01):\n",
    "    '''\n",
    "    create model\n",
    "   \n",
    "    :param I: number of items\n",
    "    :param U: number of users\n",
    "    :param K: number of units in hidden layer\n",
    "    :param hidden_activation: activation function of hidden layer\n",
    "    :param output_activation: activation function of output layer\n",
    "    :param q: drop probability\n",
    "    :param l: regularization parameter of L2 regularization\n",
    "    '''\n",
    "    x_item = Input((I,), name='x_item')\n",
    "    h_item = Dropout(q)(x_item)\n",
    "    h_item = Dense(K, kernel_regularizer=l2(l), bias_regularizer=l2(l))(h_item)\n",
    "\n",
    "    x_user = Input((1,), dtype='int32', name='x_user')\n",
    "\n",
    "    if hidden_activation:\n",
    "        h = Activation(hidden_activation)(h_item)\n",
    "    y = Dense(I, activation=output_activation)(h)\n",
    "\n",
    "    return Model(input=x_item, output=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = 'TaFeng.csv'\n",
    "     \n",
    "data   = dataProcessing()\n",
    "df     = data.readTaFeng(folder)\n",
    "new_df = data.addTransactionID(df)\n",
    "\n",
    "train, test_unf = data.nTransactionsCustomers(new_df)\n",
    "UI, itemDict = data.aggregateUserProcessing(train)\n",
    "baskets, usersTest = data.aggregateTransactionProcessing(test_unf, 4)\n",
    "\n",
    "modelRND = RecommendationModels(UI, baskets)\n",
    "modelPOP = RecommendationModels(UI, baskets)\n",
    "\n",
    "itemPoprnd, itemPrior = modelRND.itemPopularity()\n",
    "itemPoppop, itemPrior = modelPOP.itemPopularity()\n",
    "\n",
    "copyTest1 = copy.deepcopy(baskets)\n",
    "copyTest2 = copy.deepcopy(baskets)\n",
    "n = 3\n",
    "targetsRND, evidencesRND = modelRND.splitTargetEvidence(itemPoprnd, n, copyTest1, itemDict, rnd=True, pop=False)\n",
    "targetsPOP, evidencesPOP = modelPOP.splitTargetEvidence(itemPoppop, n, copyTest2, itemDict, rnd=False, pop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, test_x = prepareCDAE(UI, evidencesPOP, usersTest, itemDict)\n",
    "train_x_rnd, test_x_rnd = prepareCDAE(UI, evidencesRND, usersTest, itemDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create(I=train_x.shape[1], U=len(train_x), K=250, hidden_activation='relu', output_activation='linear', q=0.01, l=0.01)\n",
    "model.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=train_x, y=train_x,\n",
    "                   batch_size=64, nb_epoch=100, verbose=2,\n",
    "                   validation_split=0.2)\n",
    "pred = model.predict(x=test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [50, 100, 150, 200, 250]:\n",
    "\n",
    "        model = create(I=train_x.shape[1], U=len(train_x), K=i, hidden_activation='relu', output_activation='linear', q=0.01, l=0.01)\n",
    "        model.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "        model.summary()\n",
    "\n",
    "        history = model.fit(x=train_x, y=train_x,\n",
    "                           batch_size=128, nb_epoch=100, verbose=0,\n",
    "                           validation_split=0.2)\n",
    "        pred = model.predict(x=test_x)\n",
    "\n",
    "        def randomRecommend(Rhat, evidences, targets):\n",
    "            hits = []\n",
    "            hitCount= 0\n",
    "            for i in range(len(evidences)):\n",
    "                scores = np.asarray(Rhat[i])\n",
    "                scores[evidences[i]] = 0\n",
    "                recommendations = np.argsort(scores)[-3:] \n",
    "\n",
    "                for item in recommendations:\n",
    "                    if item in targets[i]:\n",
    "                        hitCount = hitCount + 1\n",
    "\n",
    "                if hitCount > 0:\n",
    "                    hits.append(1)\n",
    "                else:\n",
    "                    hits.append(0)\n",
    "                hitCount = 0\n",
    "                hitRate = sum(hits)/len(hits)\n",
    "\n",
    "            return hitRate\n",
    "\n",
    "        def modelBaskToRecoBask(evidences, targets):\n",
    "            recoEvidences = copy.deepcopy(evidences)\n",
    "            recoTargets = copy.deepcopy(targets)\n",
    "\n",
    "            for i in range(len(targets)):\n",
    "                for j in range(len(targets[i])):\n",
    "                    recoTargets[i][j] = itemDict[targets[i][j]]\n",
    "\n",
    "            for i in range(len(evidences)):\n",
    "                for j in range(len(evidences[i])):\n",
    "                    recoEvidences[i][j] = itemDict[evidences[i][j]]\n",
    "\n",
    "            return recoTargets, recoEvidences\n",
    "        pred_rnd = model.predict(x=test_x_rnd)\n",
    "        recoTargets, recoEvidences = modelBaskToRecoBask(evidencesPOP, targetsPOP)\n",
    "        recoTargetsRND, recoEvidencesRND = modelBaskToRecoBask(evidencesRND, targetsRND)\n",
    "\n",
    "        hitRateRND = randomRecommend(pred_rnd, recoEvidencesRND, recoTargetsRND)\n",
    "        hitRatePOP = randomRecommend(pred, recoEvidences, recoTargets)\n",
    "        print(hitRatePOP*100)\n",
    "        print(hitRateRND*100)\n",
    "        print('Size hidden layer: ', i)\n",
    "        print('Batch size: ', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
